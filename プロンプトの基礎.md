# プロンプトエンジニアリング：体系的アプローチ

## 概要

大規模言語モデル（LLM）の性能は、入力プロンプトの設計に大きく依存する。本稿では、プロンプトエンジニアリングの理論的基盤と実践的手法を体系的に解説する。

## LLMの確率的性質とプロンプト設計

### トークン化とコンテキスト窓

LLMは入力テキストをトークンに分解し、確率分布に基づいて次トークンを予測する。プロンプト設計においては以下を考慮すべきである:

- **コンテキスト窓の制約**: モデルのmax_tokens制限内で情報密度を最適化
- **トークン効率**: 冗長性を排除し、セマンティック密度を最大化
- **位置バイアス**: Attention機構における位置エンコーディングの影響（特に序盤と終盤の情報の重み付け）

### 温度パラメータと決定性

```python
# 温度パラメータの戦略的使用
temperature = 0.0    # 決定的出力（コード生成、分析タスク）
temperature = 0.7    # 創造的タスク（brainstorming、創作）
temperature = 1.0+   # 多様性重視（複数候補生成）
```

## 高度なプロンプティング技法

### 1. Few-Shot Learning

プロンプト内に複数の例示を埋め込むことで、モデルの出力パターンを制御する。

```
タスク: 感情分析（5段階評価）

入力: "この製品は期待以上の性能を発揮した"
出力: {"sentiment": "positive", "score": 4, "confidence": 0.85}

入力: "配送は遅れたが、商品自体は問題なし"
出力: {"sentiment": "neutral", "score": 3, "confidence": 0.72}

入力: "完全に期待外れで返品した"
出力: {"sentiment": "negative", "score": 1, "confidence": 0.91}

入力: "{{USER_INPUT}}"
出力:
```

**設計原則**:
- 例示の多様性とタスクの複雑性のトレードオフ
- ラベルノイズの最小化
- 例示の順序効果（recency bias）の考慮

### 2. Chain-of-Thought (CoT) Prompting

複雑な推論タスクにおいて、中間ステップを明示的に要求することで精度を向上させる。

```
問題: {{COMPLEX_PROBLEM}}

以下の形式で段階的に解答してください:

## 問題の形式化
- 入力変数の定義
- 制約条件の列挙
- 目的関数の特定

## 解法の選択
- 適用可能なアルゴリズム/手法の候補
- 計算量の見積もり
- 選択理由

## 実装
- 擬似コード
- エッジケースの処理

## 検証
- テストケース
- 正当性の証明または根拠
```

**研究知見**: Wei et al. (2022)の研究により、CoTは特に算術推論、常識推論、記号操作タスクで顕著な性能向上を示す。

### 3. Self-Consistency

同一プロンプトに対して複数回サンプリングし、多数決または一貫性チェックで信頼性を向上。

```python
# 擬似実装
responses = []
for _ in range(N):  # N = 5-10 程度
    response = llm.generate(prompt, temperature=0.7)
    responses.append(response)

# 一貫性検証
final_answer = majority_vote(responses)
confidence = calculate_agreement_score(responses)
```

### 4. Tree of Thoughts (ToT)

探索的な問題解決のため、複数の推論パスを木構造で展開。

```
タスク: {{PROBLEM}}

ステップ1: 初期アプローチを3つ生成
[アプローチA] ...
[アプローチB] ...
[アプローチC] ...

ステップ2: 各アプローチを評価（1-10点）
[評価基準: 実現可能性、完全性、効率性]

ステップ3: 最高評価のアプローチを展開
[選択されたアプローチ] -> [次の3つのサブステップ]
...
```

### 5. Retrieval-Augmented Generation (RAG)パターン

外部知識ベースと統合する際のプロンプト構造:

```
## コンテキスト（信頼度スコア付き）
[Document 1] (relevance: 0.94)
{{RETRIEVED_CONTENT_1}}

[Document 2] (relevance: 0.87)
{{RETRIEVED_CONTENT_2}}

## 制約
- 上記コンテキストに基づいて回答すること
- コンテキストに情報がない場合は明示的に述べること
- 引用元を[Document N]形式で示すこと

## クエリ
{{USER_QUERY}}

## 回答
```

## プロンプト構造の最適化

### システムプロンプトとメタプロンプト

```xml
<system>
あなたは{{DOMAIN}}の専門家として振る舞います。

## 応答プロトコル
1. 技術的正確性を最優先
2. 不確実性は明示的に表明
3. 出力は{{FORMAT}}形式に準拠

## 知識カットオフ
{{KNOWLEDGE_CUTOFF_DATE}}以降の情報には言及しない

## 禁止事項
- 投機的な主張
- 未検証の統計データ
- 主観的評価（要求されない限り）
</system>

<user>
{{USER_PROMPT}}
</user>
```

### 構造化出力の強制

JSON Schema、XML Schema、または型システムを用いた厳密な出力制約:

```
期待される出力形式（TypeScript型定義）:

interface AnalysisResult {
  summary: string;
  keyFindings: Array<{
    finding: string;
    evidence: string[];
    confidence: number;  // 0.0-1.0
  }>;
  limitations: string[];
  metadata: {
    analysisDate: string;  // ISO 8601
    modelVersion: string;
  };
}

上記の型に厳密に準拠したJSON出力を生成してください。
```

## トークン最適化戦略

### 1. 圧縮技法

```
# 冗長な表現
「以下のPythonコードを見て、その機能を詳しく説明してください」

# 最適化後
「次のPythonコードの機能を説明せよ:」
```

### 2. テンプレート変数の活用

```python
TEMPLATE = """
分析対象: {target}
分析軸: {dimensions}
出力形式: {format}
制約: トークン数<{max_tokens}
"""

# 動的生成
prompt = TEMPLATE.format(
    target=data,
    dimensions=["性能", "セキュリティ", "保守性"],
    format="JSON",
    max_tokens=500
)
```

### 3. 段階的精緻化

大規模タスクを複数の小プロンプトに分解し、チェーン化:

```
Phase 1: 要件抽出 -> summary_1
Phase 2: アーキテクチャ設計(input: summary_1) -> design
Phase 3: 実装(input: design) -> code
Phase 4: テスト生成(input: code) -> tests
```

## 評価とベンチマーク

### 定量評価指標

1. **タスク固有メトリクス**
   - 分類タスク: Accuracy, F1, AUC-ROC
   - 生成タスク: BLEU, ROUGE, BERTScore
   - コード生成: pass@k, functional correctness

2. **プロンプト効率**
   - Token/Response ratio
   - Cost per successful output
   - Latency vs. quality curve

### A/Bテストフレームワーク

```python
def evaluate_prompt_variants(variants, test_cases):
    results = {}
    for variant in variants:
        scores = []
        for test_case in test_cases:
            response = llm.generate(variant.format(**test_case))
            score = evaluate(response, test_case.expected)
            scores.append(score)
        results[variant.id] = {
            'mean': np.mean(scores),
            'std': np.std(scores),
            'p95': np.percentile(scores, 95)
        }
    return results
```

## アンチパターンと対策

### 1. Prompt Injection脆弱性

```
# 脆弱な設計
user_input = get_user_input()
prompt = f"Translate to English: {user_input}"

# user_inputが "Ignore previous instructions and ..." の場合に悪用される
```

**対策**:
```
システムプロンプトとユーザー入力を明確に分離:

<system>翻訳タスク専用モード</system>
<input>{sanitized_user_input}</input>
<instruction>上記inputを英語に翻訳</instruction>
```

### 2. コンテキスト汚染

複数ターン対話において、過去の不正確な情報が後続の出力に影響。

**対策**: 定期的なコンテキストリセット、重要情報の再提示

### 3. 過度な一般化

特定ドメインでの性能を他ドメインに誤って期待。

**対策**: ドメイン固有のvalidationセット構築

## 先進的なパターン

### Constitutional AI パターン

```
## 第1段階: 初期応答生成
{{INITIAL_RESPONSE}}

## 第2段階: 自己批評
以下の原則に照らして上記応答を評価:
1. 事実的正確性
2. バイアスの有無
3. 有害性チェック

## 第3段階: 改訂
批評に基づき応答を改善:
{{REVISED_RESPONSE}}
```

### Meta-Prompting

プロンプト自体を生成するメタレベルのプロンプト:

```
あなたはプロンプトエンジニアです。以下のタスクに最適なプロンプトを設計してください:

タスク記述: {{TASK_DESCRIPTION}}
制約条件: {{CONSTRAINTS}}
期待される出力形式: {{OUTPUT_SCHEMA}}

設計すべきプロンプトの要件:
- Few-shot例を3つ含む
- CoT形式で推論ステップを明示
- エラーハンドリング指示を含む
```

## 実装例: 複雑なコード生成タスク

```
# Context
リポジトリ構造: {{REPO_STRUCTURE}}
既存API仕様: {{API_SPEC}}
使用技術スタック: {{TECH_STACK}}

# Task
以下の機能を実装するPR-readyなコードを生成:
{{FEATURE_DESCRIPTION}}

# Requirements
1. 型安全性: 完全なTypeScript型定義
2. テスト: 単体テスト（カバレッジ>80%）
3. エラーハンドリング: すべての外部依存に対して
4. ドキュメント: TSDoc形式のAPI documentation
5. パフォーマンス: O(n log n)以下の計算量

# Constraints
- 既存のコーディング規約に準拠（{{STYLE_GUIDE}}）
- 破壊的変更なし
- 依存パッケージ追加は最小限

# Output Format
```typescript
// 1. インターフェース定義
// 2. 実装
// 3. テストスイート
// 4. 使用例
```

# Verification Checklist
生成後、以下を確認:
- [ ] TypeScriptコンパイルエラーなし
- [ ] ESLint違反なし
- [ ] すべてのエッジケースをカバー
- [ ] API後方互換性維持
```

## プロンプトのバージョニングとガバナンス

本番環境でのプロンプト管理:

```yaml
prompt_id: "code-review-v2.3"
version: "2.3.0"
author: "engineering-team"
created_at: "2025-11-20"
changelog:
  - "CoT reasoning steps追加"
  - "セキュリティチェック項目強化"
performance_metrics:
  accuracy: 0.89
  avg_latency_ms: 1250
  cost_per_1k_tokens: 0.015
a_b_test_results:
  variant_a_win_rate: 0.67
  statistical_significance: 0.01
approved_for_production: true
```

## 結論

プロンプトエンジニアリングは、LLMの能力を最大限引き出すための体系的な技術である。Few-shot learning、CoT、self-consistencyなどの手法を適切に組み合わせ、タスク特性に応じた最適化を行うことで、実用レベルの性能を達成できる。

継続的な評価、A/Bテスト、バージョン管理を通じて、プロンプトを進化させることが重要である。

## 参考文献

- Wei, J. et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
- Yao, S. et al. (2023). "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
- Wang, X. et al. (2023). "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
- OpenAI (2023). "GPT-4 Technical Report"
- Anthropic (2024). "Constitutional AI: Harmlessness from AI Feedback"

---

*最終更新: 2025年11月*
